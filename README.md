# ğŸ Swarm

A comprehensive CLI-based agent for web browsing, automation, and deep research with LLM integration.

## âœ¨ Features

- **ğŸŒ Interactive Web Browsing**: Visible browser automation with step-by-step control
- **ğŸ¤– AI-Powered Research**: Enhanced research assistant with comprehensive 4-phase analysis
- **ğŸ” Smart Search**: Context-aware search with DuckDuckGo integration and intelligent result selection
- **ğŸ“ Content Analysis**: Extract and analyze web page content with natural language queries
- **âš¡ MCP Server**: Expose browser automation tools to LLMs via Model Context Protocol with full logging
- **ğŸ¯ Natural Language Control**: Interact with web pages using conversational commands
- **ğŸ”§ Transparent Tool Usage**: Complete visibility into MCP tool calls and results
- **ğŸ“Š Research Reports**: Generate comprehensive research reports with structured analysis

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/swarm.git
cd swarm

# Install dependencies with UV
uv sync

# Install Playwright browsers
uv run playwright install

# Install Ollama (for LLM features)
# Visit https://ollama.ai and install for your platform
# Then pull a model:
ollama pull llama3.2:latest
```

### Configuration

Create a `.env` file with your settings (copy from `.env.example`):

```bash
# LLM Configuration (for AI features)
LLM_BASE_URL=http://localhost:11434
LLM_MODEL=llama3.2:latest
LLM_API_KEY=
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=8192

# Browser Configuration (set to false for interactive mode)
BROWSER_HEADLESS=false
BROWSER_TIMEOUT=60000
BROWSER_VIEWPORT_WIDTH=1280
BROWSER_VIEWPORT_HEIGHT=720

# Web Search Configuration
SEARCH_ENGINE=duckduckgo
SEARCH_RESULTS_LIMIT=10

# Logging Configuration
LOG_LEVEL=INFO
LOG_FILE=swarm.log

# Performance Settings
MAX_CONCURRENT_REQUESTS=5
REQUEST_TIMEOUT=120
```

### Usage

#### Enhanced Research Mode (New!)

Conduct comprehensive research with AI-powered analysis:

```bash
# Basic research with 5 sources
uv run python swarm/cli/commands/research.py "best gaming headsets 2024"

# Advanced research with custom settings
uv run python swarm/cli/commands/research.py "artificial intelligence trends" --limit 8 --output ai_research.txt --verbose

# Research with visible browser (for debugging)
uv run python swarm/cli/commands/research.py "python web frameworks" --limit 5 --verbose
```

**Research Features:**
- ğŸ” **Phase 1**: Web search with duplicate URL elimination
- ğŸ“„ **Phase 2**: Source analysis with relevance scoring and content extraction
- ğŸ§  **Phase 3**: Content synthesis with key findings and theme identification
- ğŸ“ **Phase 4**: Final report generation with structured summaries

#### Interactive Mode (Recommended)

Start the intelligent interactive mode with natural language processing:

```bash
uv run swarm interactive
```

The interactive mode automatically detects if an MCP server is running and switches between:
- **ğŸ”§ MCP Mode**: Uses MCP server with full tool logging when server is detected
- **ğŸŒ Direct Mode**: Uses direct browser automation when no MCP server is running

**Natural Language Examples:**
- "Browse github.com"
- "Search for Python web scraping tutorials"
- "What is this page about?"
- "Click the login button"
- "Fill in the email field with john@example.com"
- "Select United States from the country dropdown"

#### MCP Server (For LLM Integration)

Start the MCP server to expose browser automation tools to LLMs:

```bash
# Start MCP server with full logging
uv run swarm --verbose mcp-server

# Run in background for use with interactive mode
uv run swarm --verbose mcp-server &
```

When the MCP server is running, you'll see detailed logging of all tool calls:

```
ğŸ”§ MCP Tool: start_browser_session called with headless=False
âœ… MCP Tool: start_browser_session -> success
ğŸ”§ MCP Tool: navigate_to_url called with url=https://example.com
âœ… MCP Tool: navigate_to_url -> success to Example Page
```

The MCP server provides **14 tools** to LLMs:

**Session Management:**
- `start_browser_session` - Start persistent browser session
- `close_browser_session` - Close browser and cleanup
- `get_session_status` - Check current session status

**Navigation:**
- `navigate_to_url` - Browse to specific URL
- `get_current_page_info` - Get page details and elements

**Interaction:**
- `click_element` - Click element by visible text
- `fill_input_field` - Fill form fields by label
- `select_dropdown_option` - Select dropdown options

**Content Extraction:**
- `extract_page_content` - Get page content with filtering
- `get_page_links` - List all page links
- `get_interactive_elements` - Get clickable elements

**Search:**
- `search_web` - DuckDuckGo web search
- `search_current_page` - Search within current page
- `search_and_navigate` - Search + auto-navigate

#### Basic Commands

```bash
# Browse a specific URL
uv run swarm browse https://example.com

# Search the web
uv run swarm search "Python tutorials"

# Show information
uv run swarm info
```

## ğŸ—ï¸ Architecture

### Core Components

- **ğŸ§  LLM Client**: Connects to Ollama, VLLM, or OpenAI-compatible APIs with enhanced timeout handling
- **ğŸŒ Browser Engine**: Playwright-based automation with persistent sessions and async support
- **ğŸ” Search Engine**: DuckDuckGo integration with duplicate URL elimination
- **âš¡ MCP Server**: Model Context Protocol server for LLM tool integration with full logging
- **ğŸ¨ CLI Interface**: Rich terminal interface with progress indicators
- **ğŸ”§ Smart Detection**: Automatic MCP server detection and mode switching
- **ğŸ“Š Research Assistant**: Enhanced 4-phase research system with comprehensive analysis

### Enhanced Research System

The research assistant conducts comprehensive analysis in 4 phases:

1. **ğŸ” Phase 1 - Web Search**: 
   - DuckDuckGo search with duplicate URL elimination
   - Intelligent result filtering and validation
   - Progress tracking with visual feedback

2. **ğŸ“„ Phase 2 - Source Analysis**:
   - Content extraction from each source
   - Relevance scoring based on query matching
   - Individual source summaries with retry logic
   - Error handling for inaccessible sources

3. **ğŸ§  Phase 3 - Content Synthesis**:
   - Key findings extraction from each source
   - Theme identification across all sources
   - Cross-source pattern recognition
   - Structured data organization

4. **ğŸ“ Phase 4 - Final Report Generation**:
   - Comprehensive executive summary
   - Structured key findings with evidence
   - Theme analysis with supporting sources
   - Detailed source-by-source breakdown
   - Complete research statistics

### Technical Improvements

- **ğŸ”§ Timeout Handling**: Increased timeouts (120s) and retry logic for LLM calls
- **ğŸš« Duplicate Prevention**: URL deduplication in search results
- **ğŸ“Š Token Management**: Optimized token usage with content limiting
- **âš¡ Async Operations**: Full async support eliminating threading conflicts
- **ğŸ›¡ï¸ Error Recovery**: Comprehensive error handling with fallback strategies

## ğŸ“– Usage Examples

### Comprehensive Research Workflow

```bash
# Research with detailed progress reporting
uv run python swarm/cli/commands/research.py "best wireless earbuds 2024" --limit 5 --output earbuds_research.txt --verbose

# Output includes:
# - Search results table with URLs and titles
# - Individual source analysis panels
# - Key findings extraction
# - Theme identification
# - Final structured report
# - Complete research statistics
```

### Research Workflow with MCP Logging

```python
# LLM can perform sophisticated research with full visibility:
start_browser_session()  # ğŸ”§ MCP Tool: start_browser_session -> success
search_and_navigate("latest AI research papers", auto_select=True)  # ğŸ”§ MCP Tool: search_and_navigate -> success
content = extract_page_content("machine learning trends")  # ğŸ”§ MCP Tool: extract_page_content -> success (2451 chars)
click_element("Next Page")  # ğŸ”§ MCP Tool: click_element -> success
more_content = extract_page_content("deep learning")  # ğŸ”§ MCP Tool: extract_page_content -> success
# LLM analyzes and synthesizes information
close_browser_session()  # ğŸ”§ MCP Tool: close_browser_session -> success
```

### Interactive Search Session

```bash
# Start MCP server in background
uv run swarm --verbose mcp-server &

# Start interactive mode (automatically detects MCP server)
uv run swarm interactive

# Example interaction:
ğŸ What would you like me to do?: search for best gaming mouse
ğŸ” Searching the web for: 'search for best gaming mouse'
ğŸ”§ MCP Tool [search_web] - Searching the web
ğŸ” Found 10 search results:
[Results displayed in table format]

# Navigate to a result
ğŸ What would you like me to do?: browse result 4
ğŸ”§ MCP Tool: navigate_to_url called with url=https://www.ign.com/articles/best-gaming-mouse
âœ… MCP Tool: navigate_to_url -> success to The Best Gaming Mouse: Our Top Reviewed Picks

# Search within the page
ğŸ What would you like me to do?: search for wireless gaming mouse
ğŸ” Searching within current page for: 'wireless gaming mouse'
ğŸ”§ MCP Tool: extract_page_content called with query=wireless gaming mouse
âœ… Found 5 relevant matches on this page
```

### Form Automation

```python
# Automate complex form filling with logging:
navigate_to_url("https://example.com/signup")  # ğŸ”§ MCP Tool: navigate_to_url -> success
fill_input_field("Email", "user@example.com")  # ğŸ”§ MCP Tool: fill_input_field -> success
fill_input_field("Password", "secure_password")  # ğŸ”§ MCP Tool: fill_input_field -> success
select_dropdown_option("Country", "United States")  # ğŸ”§ MCP Tool: select_dropdown_option -> success
click_element("Create Account")  # ğŸ”§ MCP Tool: click_element -> success
```

## ğŸ”§ Development

### Setup

```bash
# Install development dependencies
uv sync --group dev

# Run tests
uv run pytest

# Format code
uv run black .
uv run isort .

# Type checking
uv run mypy .
```

### Project Structure

```
swarm/
â”œâ”€â”€ cli/                 # CLI commands and interface
â”‚   â”œâ”€â”€ commands/        # Individual command handlers
â”‚   â”‚   â”œâ”€â”€ interactive.py         # Interactive mode with MCP detection
â”‚   â”‚   â”œâ”€â”€ research.py           # Research command with argument parsing
â”‚   â”‚   â””â”€â”€ research_assistant.py # Enhanced 4-phase research assistant
â”‚   â””â”€â”€ main.py         # Main CLI entry point
â”œâ”€â”€ core/               # Core configuration and exceptions
â”‚   â””â”€â”€ config.py       # Enhanced configuration with better defaults
â”œâ”€â”€ web/                # Web automation components
â”‚   â”œâ”€â”€ browser.py      # Async Playwright browser automation
â”‚   â””â”€â”€ search.py       # DuckDuckGo search with duplicate elimination
â”œâ”€â”€ llm/                # LLM client and integration
â”‚   â””â”€â”€ client.py       # Enhanced LLM client with timeout handling
â”œâ”€â”€ mcp/                # Model Context Protocol server
â”‚   â””â”€â”€ browser_server.py    # MCP server with 14 tools and logging
â””â”€â”€ utils/              # Utility functions
```

## ğŸ¤ LLM Integration

### Claude Desktop

Add to your Claude Desktop configuration:

```json
{
  "mcpServers": {
    "swarm-browser": {
      "command": "uv",
      "args": ["run", "swarm", "mcp-server"],
      "cwd": "/path/to/swarm"
    }
  }
}
```

### OpenAI API

```python
import openai
from mcp_client import MCPClient

mcp_client = MCPClient("http://localhost:8000")
response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Research Python web frameworks"}],
    tools=mcp_client.get_available_tools()
)
```

## ğŸ§ª Testing & Verification

The project includes comprehensive testing to ensure:

- âœ… **MCP Server Detection**: Correctly detects when MCP server is running
- âœ… **Tool Integration**: All 14 MCP tools work with proper logging
- âœ… **Search Functionality**: Query detection and term extraction work correctly
- âœ… **Content Analysis**: Page content extraction with search filtering
- âœ… **Error Handling**: Robust error handling and recovery
- âœ… **Research System**: 4-phase research with duplicate elimination and timeout handling

Run verification tests:

```bash
# Test MCP integration and search functionality
python -c "
from swarm.cli.commands.interactive import _check_mcp_server_running, is_search_query, extract_search_terms
print('MCP Detection:', _check_mcp_server_running())
print('Search Query:', is_search_query('search for python tutorials'))
print('Search Terms:', extract_search_terms('search for python tutorials'))
"

# Test research functionality
uv run python swarm/cli/commands/research.py "test query" --limit 2 --verbose
```

## ğŸ“š Documentation

- [MCP Integration Guide](examples/mcp_integration.md) - Comprehensive MCP server usage
- [Basic Usage Examples](examples/basic_usage.py) - Simple automation examples
- [Smart Search Demo](examples/smart_search_demo.py) - Advanced search patterns

## ğŸ› ï¸ Technologies

- **[Playwright](https://playwright.dev/python/)**: Browser automation with async support
- **[FastMCP](https://gofastmcp.com/)**: Model Context Protocol server
- **[Click](https://click.palletsprojects.com/)**: CLI framework
- **[Rich](https://rich.readthedocs.io/)**: Terminal formatting and progress indicators
- **[Pydantic](https://docs.pydantic.dev/)**: Configuration management
- **[UV](https://docs.astral.sh/uv/)**: Fast Python package manager
- **[Ollama](https://ollama.ai/)**: Local LLM integration

## ğŸš€ Recent Improvements

### v2.0 - Enhanced Research System
- **ğŸ” Duplicate URL Elimination**: Fixed search returning duplicate results
- **â±ï¸ Timeout Handling**: Resolved LLM timeout issues with retry logic
- **ğŸ“Š 4-Phase Research**: Comprehensive research workflow with structured analysis
- **ğŸ›¡ï¸ Error Recovery**: Robust error handling with fallback strategies
- **âš¡ Async Operations**: Full async support eliminating threading conflicts
- **ğŸ“ Rich Reporting**: Detailed progress reporting and structured output

### Technical Fixes
- Increased HTTP timeouts from 30s to 120s
- Enhanced token management (4096 â†’ 8192 tokens)
- Added URL deduplication in search results
- Implemented retry logic for all LLM calls
- Optimized content limiting to prevent token overflow
- Updated default model to `llama3.2:latest`

## ğŸ¤ Contributing

Contributions are welcome! Please read our contributing guidelines and submit pull requests.

## ğŸ“„ License

This project is licensed under the MIT License.

---

*Transform your web browsing and research workflows with AI-powered automation and comprehensive analysis! ğŸ* 